Evaluating Large Language Models (LLMs)

## BLEU

The quality of machine-generated translations is assessed using the BLEU (Bilingual Evaluation Understudy) score. It assesses how closely the machine translation resembles one or more human translations of the same text. Higher scores indicate higher-quality translations. The score is computed by comparing the number of matched n-grams (contiguous sequences of n words) in the two translations. The BLEU score is frequently employed in machine translation research and evaluation and has established itself as a common statistic for contrasting various translation programs.

![BLEU](/assets/img/2023-05-10-Evaluating-LLM/BLEU.png)

### Implementation


## ROUGE

The ROUGE score, which stands for Recall-Oriented Understudy for Gisting Evaluation, is a metric used to assess the caliber of text generated by computers and automatic summarization. It analyzes the similarity between the reference summaries or texts written by humans and the machine-generated n-grams (contiguous sequences of n words). Higher scores indicate better quality summaries or material. The score is obtained by comparing precision, recall, and F1 score. The ROUGE score is frequently employed in summarizing research and evaluation and has established itself as a common tool for contrasting various summary schemes.

![ROUGE](/assets/img/2023-05-10-Evaluating-LLM/ROUGE.png)

### Implementation


## METEOR

A metric called METEOR (Metric for Evaluation of Translation with Explicit ORdering) is employed to assess the caliber of translations or summaries produced by automated means. Unlike other metrics like BLEU and ROUGE, which emphasize n-gram overlap, METEOR also takes into account synonymy and word sense in addition to n-gram overlap. To compare the machine-generated text with the reference text, it combines precision, recall, and alignment-based metrics. Metrics like unigram matching, synonymy matching, and paraphrase matching can all have their relative weights adjusted inside METEOR to better suit the demands of a given job or application. The scale goes from 0 to 1, with higher ratings indicating translations or summaries of higher quality. METEOR is frequently used in research and assessment on machine translation and summarization.

In METEOR, recall is given a higher weight than accuracy when calculating the Harmonic mean of the unigram precision and recall. Giving memory more weight enables us to better grasp how much ground truth was generated as output throughout the generating process. Below, the precise mathematical formulation is displayed:

![METEOR](/assets/img/2023-05-10-Evaluating-LLM/METEOR.png)

Additionally, METEOR contains a concept known as the "Chunk Penalty" that takes into account chunks (consecutive words) as well as unigram overlap in order to impose some sort of ordering. It is calculated as shown below and is symbolized by the letter "p":

![chunk-penalty](/assets/img/2023-05-10-Evaluating-LLM/chunk-penalty.png)

The number of chunks in the candidate that also appear in the reference and unigrams in the candidate phrase are indicated here by the letters c and um, respectively. In order to calculate the METEOR score (M), multiply the factor "p" by the F-score, as illustrated below.

![meteor-score-calculation](/assets/img/2023-05-10-Evaluating-LLM/meteor-score-calculation.png)

### Implementation

## BERTScore

### Implementation
